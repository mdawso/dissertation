\chapter{Literature Review}

This chapter contains a review of traditional approaches to AI in video games.
It then explores RL and deep learning fundamentals, algorithms and data structures, as well as some of their existing applications to video games.

\section{Traditional approaches to AI in video games}

This section explores some existing traditional methods of implementing AI in video games.

\subsection{Finite State Machines}

A Finite State Machine (FSM) is a computational model that has been foundational in video game AI development for decades. 
FSMs represent an agent's behaviour as a set of discrete states, with well-defined transitions between these states triggered by specific conditions or events \cite{spiceworks_fsm}. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/fsm_example.png}
    \caption{A simple FSM for an enemy NPC}
    \label{fig:fsm}
\end{figure}

Each state encapsulates a particular behaviour or action pattern, while transitions define the rules governing when an agent should change its behaviour. 
The simplicity and predictability of FSMs make them particularly suitable for controlling NPCs with straightforward behavioural patterns, as they are computationally efficient and easily debuggable.
However, FSMs face significant limitations as complexity increases: the number of states and transitions can grow exponentially, leading to the "state explosion" problem that makes maintenance challenging.
Additionally, FSMs struggle with handling concurrent behaviours and can appear rigid when compared to more dynamic AI approaches. 
Despite these limitations, FSMs remain prevalent in game development due to their intuitive implementation and reliable performance for many common AI tasks.

\subsection{Behaviour Trees}

Behaviour Trees (BTs) represent a significant advancement over FSMs in game AI architecture, offering a hierarchical, modular approach to decision-making. 
Originally developed for robotics and adopted by the game industry in titles like Halo 2 \cite{simulacrum}, BTs organize agent behaviours into a tree structure where leaf nodes represent atomic actions and internal nodes control flow through various composites such as sequences, selectors, and parallels. 
This structure enables developers to create complex, reusable behaviour patterns that can be visually represented and intuitively understood. 
Unlike FSMs, BTs naturally handle concurrent actions and gracefully manage behaviour prioritization through their hierarchical evaluation.
BTs excel at creating responsive AI that can react to changing game conditions while maintaining coherent behaviour patterns. 
They facilitate an incremental development approach, allowing designers to progressively refine AI by adding branches without disrupting existing functionality.
While BTs require more initial design consideration than FSMs, their scalability, maintainability, and ability to represent sophisticated decision-making logic have made them the standard approach for contemporary game AI systems, particularly in action, strategy, and open-world games where adaptable NPC behaviour is critical to player experience.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/bt.png}
    \caption{A simple BT for an enemy NPC}
    \label{fig:bt}
\end{figure}

\subsection{Goal-Oriented Action Planning}

Goal-Oriented Action Planning (GOAP) represents a more dynamic approach to AI decision-making compared to FSMs and BTs. 
GOAP employs principles from automated planning and means-end analysis to create AI agents that formulate plans to achieve specific goals. Unlike more rigid systems, GOAP agents dynamically determine action sequences by considering current world states, available actions with preconditions and effects, and desired goal states.
In a GOAP system, each action is associated with both preconditions (requirements that must be satisfied before the action can be taken) and effects (how the action changes the world state).
The AI agent uses planning algorithms, commonly A* search, to find the optimal sequence of actions that transforms the current state into the goal state. 
This approach allows NPCs to solve problems creatively and adapt to unexpected changes in the game environment.
GOAP gained prominence through its implementation in F.E.A.R. (2005) \cite{thompson2020fear}, where it produced enemies capable of contextually appropriate tactical behaviours like seeking cover, flanking the player, and coordinating with allies. 
The system's strength lies in its separation of goals (what the agent wants to achieve) from the specific methods to achieve them.
While GOAP offers exceptional adaptability and problem-solving capabilities, it comes with higher computational costs and increased implementation complexity compared to FSMs and BTs. Despite these challenges, GOAP remains valuable for games requiring sophisticated AI that can respond intelligently to dynamic and unpredictable gameplay scenarios.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/goap.png}
    \caption{Example of a GOAP planning sequence}
    \label{fig:goap}
\end{figure}

\section{Reinforcement Learning}

This section explores fundamental RL concepts and algorithms.

\subsection{RL Fundamentals and the Markov Decision Process}

Reinforcement Learning (RL) represents a departure from traditional game AI approaches by focusing on learning optimal behaviours through trial and error interaction with an environment. 
Unlike FSMs, BTs, or GOAP systems that rely on pre-programmed rules, RL agents improve their decision-making capabilities through experience.

At its core, RL is formalized as a Markov Decision Process (MDP) consisting of:
\begin{itemize}
    \item A set of states $S$ representing all possible situations an agent may encounter
    \item A set of actions $A$ that the agent can take
    \item Transition probabilities $P(s'|s,a)$ defining the likelihood of moving to state $s'$ after taking action $a$ in state $s$
    \item A reward function $R(s,a,s')$ providing feedback on the quality of decisions
    \item A discount factor $\gamma \in [0,1]$ determining the importance of future rewards
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/mdp_framework.png}
    \caption{A simple MDP Graph example}
    \label{fig:mdp-framework}
\end{figure}

S Nodes represent states and A nodes represent actions. 
Some actions can result in more than one state, the transition probabilities are marked in black. 
Some actions result in a positive or negative reward, marked in red.

The agent's objective is to learn a policy $\pi$ that maps states to actions in a way that maximizes the expected cumulative reward. 
This optimisation process balances immediate rewards against long-term consequences, addressing the fundamental exploration-exploitation dilemma: whether to capitalize on known good strategies or explore new possibilities that might yield better results.
RL algorithms generally fall into three categories: value-based methods (like Q-learning), policy-based methods (such as policy gradients), and actor-critic approaches that combine aspects of both. 
The choice of algorithm depends on factors including the complexity of the state space, whether the environment is fully observable, and computational constraints.
Unlike traditional AI techniques, RL offers adaptability to unexpected situations and can discover novel strategies beyond designer expectations. 
However, these advantages come with challenges including high sample complexity (requiring many environment interactions), difficulty in specifying appropriate reward functions, and potential convergence to suboptimal solutions when trained in limited scenarios.

\subsection{The Bellman equation}
The Bellman equation provides a mathematical foundation for solving Markov Decision Processes (MDPs) by establishing recursive relationships between value functions of different states. 
It serves as the cornerstone of many RL algorithms.
The Bellman equation defines the optimal value function $V^*(s)$ for each state $s$ as:
$$V^*(s) = \max_{a} \left[ R(s,a) + \gamma \sum_{s'} P(s'|s,a) V^*(s') \right]$$
This recursive formulation elegantly captures the principle that the value of a state equals the reward obtained from the best action plus the discounted expected value of the successor state. For policies, the Bellman expectation equation becomes:
$$V^{\pi}(s) = \sum_{a} \pi(a|s) \left[ R(s,a) + \gamma \sum_{s'} P(s'|s,a) V^{\pi}(s') \right]$$
Similarly, the action-value function $Q^*(s,a)$ represents the expected return starting from state $s$, taking action $a$, and thereafter following the optimal policy:
$$Q^*(s,a) = R(s,a) + \gamma \sum_{s'} P(s'|s,a) \max_{a'} Q^*(s',a')$$
Many RL algorithms iteratively apply these equations to converge to optimal solutions. Value iteration repeatedly updates state values using the Bellman optimality equation until convergence. Policy iteration alternates between policy evaluation (calculating values for the current policy) and policy improvement (selecting better actions based on updated values).
While solving the Bellman equation exactly requires complete knowledge of the environment's dynamics (transition probabilities and rewards), most practical RL algorithms like Q-learning approximate solutions through sampling and iterative updates, enabling agents to learn optimal policies through experience rather than explicit environment models.

\subsection{Deep Reinforcement Learning}

Deep Reinforcement Learning (DRL) combines classical RL with deep neural networks to handle high-dimensional state spaces that would be intractable with traditional tabular methods. 
This integration enables RL to operate effectively in complex environments with visual inputs, continuous action spaces, and intricate state representations common in modern video games.
The breakthrough Deep Q-Network (DQN) algorithm, introduced by DeepMind in 2015, demonstrated superhuman performance on Atari games using only pixel inputs and game scores. DQN employs several key innovations including:

\begin{itemize}
    \item Experience replay, which stores and randomly samples past experiences to break correlation between sequential samples
    \item Target networks that stabilize training by reducing moving target problems
    \item Convolutional neural networks that process visual information effectively
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/dqn_architecture}
    \caption{Architecture of a Deep Q-Network}
    \label{fig:dqn-architecture}
\end{figure}

\subsubsection{Experience Replay}

Experience replay is a fundamental technique in deep reinforcement learning that addresses the challenge of correlated samples in sequential learning. 
While traditional online RL updates parameters based on immediate experiences, experience replay stores transitions $(s_t, a_t, r_t, s_{t+1})$ in a buffer and samples them randomly during training.

This mechanism provides several crucial benefits:
\begin{itemize}
    \item It breaks temporal correlations between consecutive training samples that can lead to unstable learning
    \item It increases data efficiency by allowing experiences to be used multiple times
    \item It reduces the variance of updates by averaging over different states and actions
\end{itemize}
The replay buffer typically maintains a fixed-size queue that stores the $N$ most recent experiences. 
During the learning phase, mini-batches of transitions are randomly sampled from this buffer to update the neural network parameters, following:
$$Q(s,a) \leftarrow Q(s,a) + \alpha[r + \gamma\max_{a'}Q(s',a') - Q(s,a)]$$
More sophisticated variants include prioritized experience replay, which samples important transitions more frequently based on their temporal-difference error, 
and hindsight experience replay, which retrospectively generates additional training signals from failed attempts.


\subsubsection{Epsilon Decay}

Epsilon-decay represents a critical strategy in exploration-exploitation balance for reinforcement learning algorithms like DQN. 
During training, agents follow an $\epsilon$-greedy policy where they select the highest-valued action with probability $1-\epsilon$, and choose a random action with probability $\epsilon$.
Epsilon decay gradually reduces this exploration parameter over time according to a schedule:
$$\varepsilon_t = \varepsilon_{\text{end}} + (\varepsilon_{\text{start}} - \varepsilon_{\text{end}})e^{-\lambda t}$$
where $\varepsilon_{\text{start}}$ is the initial value (often near 1.0), $\varepsilon_{\text{end}}$ is the minimum value (typically 0.01-0.1), and $\lambda$ determines the decay rate.
This approach ensures thorough exploration of the environment initially, when the agent's knowledge is limited, while gradually transitioning to exploitation of learned strategies as training progresses. 
Properly tuned epsilon decay significantly impacts learning efficiency and final policy quality, particularly in environments with sparse rewards or complex optimal strategies.