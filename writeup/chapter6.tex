\chapter{Evaluation}

This chapter presents an evaluation of the implemented system, focusing on functionality and performance.

\section{Functional Evaluation}
All implemented components of the system function as intended. The game mechanics, AI interaction, and interface elements operate correctly with no significant bugs or issues observed during testing.

\section{AI Performance Evaluation}
To evaluate the performance of a trained model and demonstrate emergent intelligent behavior, I employed the following evaluation methodology:
I trained a model for 10000 iterations on the pillars map type, 10000 on the snake map type, and 10000 on the platforms map type for 30000 total iterations.
The model was then tested for 100 iterations on each map type, noting it's success rate and average completion time.
From the fully trained model, we should see high success rate and consistent completion time.

\subsection{Results}

\begin{table}[ht]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Map Type} & \textbf{Success Rate (\%)} & \textbf{Average Completion Time (s)} \\
\hline
Pillars & 98 & 3.1 \\
\hline
Snake & 99 & 3.0 \\
\hline
Platforms & 22 & 4.2 \\
\hline
\end{tabular}
\caption{Performance of trained model across different map types}
\label{tab:model_performance}
\end{table}

The results show strong performance on the pillars and snake maps, with success rates of 98\% and 99\% respectively. However, the model struggled significantly with the platforms map type, achieving only a 22\% success rate. 
This suggests that the presence of gaps in the floor, and the kill plane in the platforms environment creates challenges that the model has difficulty generalizing to, despite substantial training.
I also noticed that after training on the platforms map, the model developed a tendency to jump more than it needed to. This carried into the other maps and tended to harm its completion time.
This suggests that there is a degree of model instability. Future work could be done on the reward structure to remedy this
I also noted that the model is usually significantly faster at completing the maps than human players can. This is due to the model being able to perform frame perfect inputs (eg: jumping the same frame it hits the ground) 
that a human could not do consistently. This harms the final implementation where the model sets a time to beat for a human.

\section{Conclusion}

The evaluation results demonstrate that this project successfully achieved its primary objectives. 
The reinforcement learning model was effectively implemented within the Godot game engine environment, producing agents capable of navigating complex terrain and completing objectives autonomously. 
The trained model exhibited meaningful improvement with increased training iterations, demonstrating the efficacy of the reinforcement learning approach in this context.
Performance metrics reveal that the models achieved high success rates on two of the three map types, with completion times that indicate efficient pathfinding capabilities. 
The combined training approach proved particularly valuable for enhancing generalisation across different environmental challenges. 
These outcomes validate the core premise that reinforcement learning can produce capable game agents through progressive training.
However, several limitations were identified that warrant future improvement. 
Most notably, the models struggled significantly with the platforms map type, where the environmental hazard of the kill plane posed a greater challenge than in other environments. 
The substantially lower success rate in this map type (peaking at 22\%) indicates that more sophisticated training approaches may be necessary for handling complex failure conditions.
Additionally, the agents consistently completed tasks faster than human players would typically achieve, which presents challenges for balanced gameplay implementation. 
While computational efficiency is generally desirable, agents that move unnaturally quickly or efficiently may not provide satisfying gameplay experiences when incorporated into competitive or cooperative scenarios with human players.
Future work should focus on addressing these limitations through more specialised training techniques for hazardous environments and potentially incorporating mechanisms to modulate agent performance to better match human capabilities in gameplay scenarios.